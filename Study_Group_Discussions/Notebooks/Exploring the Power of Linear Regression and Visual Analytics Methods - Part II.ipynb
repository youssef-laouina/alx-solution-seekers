{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c49828-21be-440c-97b6-a1468e25c402",
   "metadata": {},
   "source": [
    "# Solution Seekers Group\n",
    "\n",
    "Lead of the Study Group Discussion: **Badr Bensassi**\n",
    "\n",
    "Author: **Youssef Laouina**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2529314-500a-4121-bc3a-55ce83dd5bfe",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff191d57-48bb-4454-a98a-c4326ca69cac",
   "metadata": {},
   "source": [
    "> **“All Models are wrong, but some are useful.”**\n",
    "\n",
    "*George Box*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2e063-c371-402f-a23b-23fc54df0691",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89395d66-5b0b-4fa9-8bff-c73dbdc39bc2",
   "metadata": {},
   "source": [
    "Importing our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a47609-b19f-4f62-9aea-95a2575f3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16bcb1-fea7-4db1-8f53-af305ff14bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc122471-eba1-4352-83a3-9ea6b0901200",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f251d11-8449-4b7b-bde8-d9f4c69bd698",
   "metadata": {},
   "source": [
    "## Importing our data into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fbfeb-8e59-4419-9ad0-7eb270d02c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/tirthajyoti/Machine-Learning-with-Python/master/Datasets/USA_Housing.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ac0c9-8772-4924-a174-81904993d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7834a2d-2e28-463b-bd7f-423db081c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db0719-ad01-45f7-89b0-57670edc4713",
   "metadata": {},
   "source": [
    "## Clean our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3513b9-65e2-4e15-b45c-b2d625e9919f",
   "metadata": {},
   "source": [
    "### Clean feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd09032-606a-477d-9602-48af566ac470",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = df.columns.str.replace('.', '').str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e265c3d-205d-4bc6-a6c0-14ba461cf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703a93d-25e6-4e6b-88c2-11ebb911e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1b8cb-4053-4b7d-86fc-2abaeca854a5",
   "metadata": {},
   "source": [
    "### Drop irrelevant observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350dca64-e0e2-442a-9112-f556fdef4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9cbf6-0df0-463d-8630-78c98a4eb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Address'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e307aa-15d8-48ee-9efe-8bff85b83d15",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c2a06-1ce5-4588-a091-d00b488acfcf",
   "metadata": {},
   "source": [
    "We will drop records that contains ***?PO*** as they don't align with our model's target area, potentially introducing irrelevant information.\n",
    "\n",
    "* **?PO**: Stands for \"Post Office,\" which is used for U.S. Navy, Army installations stationed overseas, or Coast Guard installations that are deployed or stationed overseas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Address'].str.contains('PO') == True]['Address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030720db-2eba-4edc-ba0e-e6fd824460ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_drop = df[(df['Address'].str.contains('PO') == True)]['Address'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d80914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index_drop, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5775ad-b8ca-4390-bc42-424fda0a9ad6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37756b83-cbb6-4af6-b1d9-862a589d83c9",
   "metadata": {},
   "source": [
    "### Extract City & State Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb295e56-8a58-43df-bc29-0d87922dea4e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a948feb-d432-418c-a9c6-bf9965b52ea7",
   "metadata": {},
   "source": [
    "Now we will extract the **city** and **state** code from `Address`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1dfbf-a1dd-4c09-93a2-4a821f56589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city-state'] = df['Address'].str.split('\\n', expand=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318278c-9bf1-469b-8401-0f3797f0a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'] = df['city-state'].str.split(', ', expand=True)[0]\n",
    "\n",
    "df['state_code'] = df['city-state'].str.split(', ', expand=True)[1].str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8b5b5-0b57-4309-8c3b-c6ebb3297636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaebffa-371b-47cc-bb90-4f833eb2ecc4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02843235-2e81-4efe-9a27-8d37891f25db",
   "metadata": {},
   "source": [
    "### Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71cfdf9-a639-4761-af4b-8dcba6bce5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851ef2d-e1f0-450a-8209-09519c3e2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Address', 'city-state'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b30775-8532-4370-8089-62f9339f28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957d514-eec5-4715-bf71-8c34ac54816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d640401-ed2e-40ef-b8da-a0759809b4f6",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65bcb7-3b24-44c9-85ab-653b06146e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = df.drop(columns=['Price', 'city'])\n",
    "target_vector = df['Price']\n",
    "\n",
    "print(f\"Feature Matrix: {feature_matrix.shape}\",\n",
    "      f\"Target Vector: {target_vector.shape}\",\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03fdf9-9c8d-4373-995c-8279fe1e94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix,\n",
    "                                                    target_vector,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ec415-fa4d-49c3-8846-30a9b45ce038",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a5bdd-2876-4afb-b4e0-99dac2e8dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = y_train.mean()\n",
    "y_pred_baseline = [y_mean] * len(y_train)\n",
    "mse_baseline = mean_squared_error(y_train, y_pred_baseline)\n",
    "\n",
    "print(\"Mean house price:\", round(y_mean, 2))\n",
    "\n",
    "print(\"Baseline RMSE:\", round(np.sqrt(mse_baseline), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107859ec-c205-4edd-8858-122098bdb7ba",
   "metadata": {},
   "source": [
    "## Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4b517-c474-41a1-ae8b-31c975e111da",
   "metadata": {},
   "source": [
    "If we try to fit a LinearRegression predictor to your training data at this point, we'll get an error that looks like this:\n",
    "\n",
    "`ValueError: could not convert string to float`\n",
    "\n",
    "What does this mean? When we fit a linear regression model, we're asking scikit-learn to perform a mathematical operation. The problem is that our training set contains city and state information in non-numerical form. In order to create our model we need to **encode** that information so that it's represented numerically.\n",
    "\n",
    "The good news is that there are lots of transformers that can do this. Here, we'll use the one from the ***Category Encoders*** library, called a **OneHotEncoder**.\n",
    "\n",
    "Before we build include this transformer in our model, let's explore how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de788d79-3664-4ba6-9455-69296b89038c",
   "metadata": {},
   "source": [
    "### Encode non-numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4869dce-75ba-404f-bf42-674956801905",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(use_cat_names=True)\n",
    "ohe.fit(X_train)\n",
    "\n",
    "XT_train = ohe.transform(X_train)\n",
    "\n",
    "print(XT_train.shape)\n",
    "XT_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4976048-ae51-4000-bce0-9dac0f531e02",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aeae39-9fe5-423d-a059-36406fcae5c9",
   "metadata": {},
   "source": [
    "### Make pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4838f167-c743-43fe-96e6-afdbf6f27621",
   "metadata": {},
   "source": [
    "Now we will create a pipeline named model that contains a OneHotEncoder transformer and a LinearRegression predictor. Then fit our model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b309e-e2c0-4974-bb28-95344145ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78043c5-21a4-48db-95c8-cb6e10eab310",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lm_train = model.predict(X_train)\n",
    "y_pred_lm_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800bb3f0-6170-418d-80fb-c719bc1964c9",
   "metadata": {},
   "source": [
    "## Evalute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516686a3-1ba3-4926-9758-1072e2214a7f",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379962c-de62-4c94-9b16-afd8b3356f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_lm_train = mean_squared_error(y_train, y_pred_lm_train)\n",
    "mse_lm_test = mean_squared_error(y_test, y_pred_lm_test)\n",
    "\n",
    "print(\"MLR metrics: \")\n",
    "print(\"\\tTraining MSE:\\t\", round(mse_lm_train, 2))\n",
    "print(\"\\tTraining RMSE:\\t\", round(np.sqrt(mse_lm_train), 2))\n",
    "print()\n",
    "print(\"\\tTesting MSE:\\t\", round(mse_lm_test, 2))\n",
    "print(\"\\tTesting RMSE:\\t\", round(np.sqrt(mse_lm_test), 2))\n",
    "print()\n",
    "print(\"\\tR-squared:\\t\", round(r2_score(y_true=y_test, y_pred=y_pred_lm_test), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16feffad-28ed-43f1-bd82-437396d8293e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e3c09-b30f-41b4-8afd-9bb9dbaff45e",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02de064-e5dd-4c4e-b839-55389111f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = model.named_steps['linearregression'].intercept_\n",
    "coefficients = model.named_steps['linearregression'].coef_\n",
    "print(\"Number of coefficients:\", len(coefficients))\n",
    "print(\"Coefficients: \", coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bb430-e1b2-470d-a758-8156873ff31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = model.named_steps['onehotencoder'].get_feature_names()\n",
    "print(\"Features len:\", len(feature_names))\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99cc70-db20-4283-b30c-b4e01047f46b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029fcef8-a84f-48dc-8a6f-7babab522f45",
   "metadata": {},
   "source": [
    "We will create a pandas Series named `feat_imp` where the index is our `features` and the values are our `coefficients`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f93c7-9c58-4cf2-971f-0bc366d6d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(coefficients ,index=feature_names)\n",
    "feat_imp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc46e8-fad6-4ced-b32d-a0467724769d",
   "metadata": {},
   "source": [
    "Now we will create a horizontal bar chart that shows top 15 coefficients for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97306119-ec6a-40b9-b4d4-2cb94943ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp.sort_values(key=np.abs).tail(15).plot(kind='barh')\n",
    "plt.xlabel('Importance [USD]')\n",
    "plt.ylabel('Features/Predictors')\n",
    "plt.title('Feature Importance for House Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85bb2b-5540-436e-9815-d78a5d1f75b2",
   "metadata": {},
   "source": [
    "Looking at this bar chart, we can see that `Avg. Area House Age` and `Avg. Area Number of Rooms` are the two features having the greater weights importance, while the other features have a small effect on the prediction power of our model.\n",
    "\n",
    "The analysis of **Feature Importance** can help us determine the most influential features in our model and possibly reduce the complexity of our model, but it is not enough to conclude if a feature has a significant effect on our model or not.\n",
    "\n",
    "We now have 64 predictor variables to choose from, so we need a way of guiding us to choose the best ones to be our predictors.\n",
    "\n",
    "One way is to look at the correlations between the `Price` and each variable in our dataset and select those with the strongest correlations - both positive and negative.\n",
    "\n",
    "We also need to consider how significant those features are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330580fb-c94f-429e-9e79-5c5ad7bb8a52",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90b4dd-5580-485b-9964-578da1202a11",
   "metadata": {},
   "source": [
    "## By correlation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597762e3-af66-4019-a9ab-859aaa69579e",
   "metadata": {},
   "source": [
    "Prepare our dataframe with the encoded state codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e6679-57c7-4cea-a7a5-45bf6d99f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_all = df.drop(columns='city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d001d7-0d48-4a50-8b56-f251f143df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(use_cat_names=True)\n",
    "ohe.fit(feature_matrix_all)\n",
    "\n",
    "df_encoded = ohe.transform(feature_matrix_all)\n",
    "\n",
    "print(df_encoded.shape)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df976f-ad4d-416f-8d29-00a73076c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_encoded.corr()['Price']\n",
    "\n",
    "correlation_df = pd.DataFrame(corr, index=corr.index).sort_values(by='Price', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d133d0-c237-4f35-9f20-83608ee1a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becb9d4-73a9-4882-8d74-dc44be0a4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df = correlation_df.drop('Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f453fa-b996-43a1-92b8-cadf4b1d6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9591c-4c54-4129-a807-21a016deb087",
   "metadata": {},
   "source": [
    "### Create and visualise the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e20976-ee88-4cd4-9b9a-c26492b95706",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "sns.barplot(y=correlation_df.index ,x=correlation_df['Price'], ax=ax, orient='h')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25dd75-257f-48d5-8595-0b8f7236be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_titles = corr.index.tolist()\n",
    "column_titles.remove('Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282062a-49d1-486d-af10-2649b0b55090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary of correlation coefficibinents and p-values\n",
    "dict_cp = {}\n",
    "\n",
    "for col in column_titles:\n",
    "    p_val = pearsonr(df_encoded[col], df_encoded['Price'])[1]\n",
    "    \n",
    "    dict_cp[col] = {\n",
    "        'Correlation_Coefficient': correlation_df.loc[col].values[0],\n",
    "        'P_Value': p_val\n",
    "    }\n",
    "\n",
    "df_cp = pd.DataFrame(dict_cp).T\n",
    "\n",
    "df_cp_sorted = df_cp.sort_values(by='P_Value')\n",
    "\n",
    "df_cp_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7edeaf-0304-46bd-9afc-597f472f1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_level = 0.05\n",
    "\n",
    "df_cp_sorted[df_cp_sorted['P_Value'] <= significance_level]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa603e44-e9f3-43d7-a1e5-b3d1b0ed2070",
   "metadata": {},
   "source": [
    "> At a significance level of 5%, we would filter our predictors to only include 10 features instead of 64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a2a05-cf62-42ca-9408-559be47b0eea",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215c506-f4f8-4921-9eb0-27d817d0b006",
   "metadata": {},
   "source": [
    "### Understanding the p-value in correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69cee23-a284-4166-971a-40c0ce71c9d1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019f268-1a30-4f9c-9286-ff8309c3d404",
   "metadata": {},
   "source": [
    "The p-value is a statistical measure that helps assess the significance of the correlation observed in a dataset. It quantifies the probability of obtaining a correlation as extreme as the one observed, assuming that the variables are actually uncorrelated (null hypothesis). Here's how the calculation generally proceeds:\n",
    "\n",
    "1. **Compute the Pearson correlation coefficient:** This measures the linear relationship between two variables, ranging from -1 to 1.\n",
    "\n",
    "2. **Assume the null hypothesis:** The null hypothesis states that there is no significant correlation between the variables in the population.\n",
    "\n",
    "3. **Generate a distribution of correlations under the null hypothesis (e.g., using bootstrapping):** This involves simulating many datasets with the same size as the original dataset but with uncorrelated variables. The Pearson correlation coefficient is calculated for each simulated dataset.\n",
    "\n",
    "4. **Calculate the p-value using the Cumulative Distribution Function (CDF):** The p-value is determined by comparing the observed correlation coefficient from the original dataset with the distribution of correlation coefficients generated under the null hypothesis. It represents the probability of obtaining a correlation coefficient at least as extreme as the observed one, assuming the null hypothesis is true.\n",
    "\n",
    "5. **Interpretation:** A small p-value (typically less than 0.05) suggests that the observed correlation is unlikely to have occurred by chance alone, leading to the rejection of the null hypothesis and indicating a statistically significant correlation.\n",
    "\n",
    "So, the p-value helps quantify the likelihood that the observed correlation in the dataset is due to random chance versus a true underlying relationship between the variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd030e-1a16-4283-b46b-2c51ee236ede",
   "metadata": {},
   "source": [
    "## By variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db084712-8f5a-4ab2-8387-3df0966a602d",
   "metadata": {},
   "source": [
    "Variance Thresholds remove features whose values don't change much from observation to observation.\n",
    "\n",
    "The objective here is to remove all features that have a variance lower than the selected threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1497f1-d6c6-4f1b-8408-89aa75c31a8a",
   "metadata": {},
   "source": [
    "**Note:** Variance is dependent on scale, so the features will have to be ***normalized*** before implementing variance thresholding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd87d19-610b-47dd-8510-553b3ed2a369",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e58fa-57e1-4100-b221-7a0d9c6677b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into independent (X) and dependent (y) variables\n",
    "\n",
    "X_data = df_encoded.drop(columns='Price')\n",
    "y_data = df_encoded['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7e021-29cf-4f46-b4ed-854bd6e91e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_data)\n",
    "X_normalized = pd.DataFrame(X_scaled, columns=X_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ae3db-3427-4921-8fc0-7cb65755e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a4fec-d269-449c-8b0c-5b6fd3ae98be",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b682c-6d2e-4e1e-b38e-695be636a768",
   "metadata": {},
   "source": [
    "### Perform variance selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3822b2f1-44cd-4305-81ef-0fdb69be6db7",
   "metadata": {},
   "source": [
    "Variance Threshold in Scikit Learn\n",
    "\n",
    "To implement Variance Threshold in Scikit Learn we have to do the following:\n",
    "\n",
    "* Import and create an instance of the VarianceThreshold class\n",
    "* Use the .fit() method to select subset of features based on the threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c06151-3e81-4641-808f-57182c0501c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7944f1-36c0-4c0c-bd3d-80821ef02613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VarianceThreshold object\n",
    "selector = VarianceThreshold(threshold=0.01879)\n",
    "\n",
    "# Use the object to apply the threshold on data\n",
    "selector.fit(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768116a-afbc-4ff5-981f-05e888e4ae64",
   "metadata": {},
   "source": [
    "The Variance Threshold has been applied to the data. Let's look at the calculated variance for each predictive variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410362f-f403-40be-a833-42955bc881e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column variances\n",
    "column_variances = selector.variances_\n",
    "\n",
    "vars_dict = [\n",
    "    {\n",
    "        \"Variable_Name\": c_name,\n",
    "        \"Normalized_Variance\": c_var\n",
    "    }\n",
    "    for c_name, c_var in zip(X_normalized.columns, column_variances)\n",
    "]\n",
    "\n",
    "df_vars = pd.DataFrame(vars_dict)\n",
    "df_vars.sort_values(by='Normalized_Variance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9687d5-c76a-46a8-ae73-91028ade95d8",
   "metadata": {},
   "source": [
    "The above table shows the variances of the individual columns before any threshold is applied. It allows us to **revise our initial variance threshold** if we feel that we might exclude important variables.\n",
    "\n",
    "\n",
    "Next we need to **extract the results** and use them to **select our new columns** - which form a subset of all the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213f408-ab27-4ae1-8f75-f19127c42f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "X_selected = X_normalized[X_normalized.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "# Save variable names for later\n",
    "X_var_names = X_selected.columns\n",
    "\n",
    "# View first few entries\n",
    "X_selected.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9960a8-4eb8-4597-884d-90e01556897b",
   "metadata": {},
   "source": [
    "#### Treashold Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146fe12-f589-494a-a9ae-821b64112a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "treashold_range = np.linspace(start=0, stop=0.03, num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135395c-214f-4842-a6fa-a353611ace0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunning of the treashold:\n",
    "\n",
    "for i in treashold_range:\n",
    "    treashold = np.round(i, 5)\n",
    "    selector = VarianceThreshold(threshold=treashold)\n",
    "    \n",
    "    selector.fit(X_normalized)\n",
    "    feat_num = selector.get_support(indices=True).shape[0]\n",
    "\n",
    "    print(f\"With threshold = {treashold}:\\n\\tNumber of features is {feat_num}\\n\")\n",
    "\n",
    "    if feat_num == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbfec5c-a4e3-4ad1-a2d6-7f1f39f695be",
   "metadata": {},
   "source": [
    "~ Y.L"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
