{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a426f5fc-eabc-427a-852e-1eeaa69c18ee",
   "metadata": {},
   "source": [
    "# Solution Seekers Group\n",
    "\n",
    "Lead of the Study Group Discussion: **Badr Bensassi**\n",
    "\n",
    "Author: **Youssef Laouina**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba8af5f-e792-4899-ac63-1c8dafb5be23",
   "metadata": {},
   "source": [
    "> **“In summary, both intuition and empirical evidence suggest that when you have heteroscedasticity, ordinary least squares standard errors, confidence intervals, and hypothesis tests are unreliable. The heteroscedasticity-robust standard errors can provide more reliable inferences.”**\n",
    "\n",
    "*Francis J. Anscombe*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b04b9-a69e-4958-927d-3836e44b1c3b",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4542bc9e-5d44-4866-8b0a-d72ee9d5b5a9",
   "metadata": {},
   "source": [
    "## Define heteroscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390827e-2f37-4ecb-b328-8fd13e22d81b",
   "metadata": {},
   "source": [
    "**Understanding Heteroscedasticity in Statistical Analysis**\n",
    "\n",
    "Heteroscedasticity is a term used in statistics to describe a situation where the variability of a variable is unequal across the range of values of a second variable that predicts it. It comes from the Greek words \"hetero,\" meaning different, and \"skedasis,\" meaning dispersion or spreading.\n",
    "\n",
    "In the context of statistical analysis, particularly in machine learning using Ordinary Least Squares (OLS), the concept of heteroscedasticity challenges the assumption made by OLS that the variance of the error term remains constant. OLS assumes that for all observations, the variance of the error term, denoted as ε, is consistent, a condition known as homoscedasticity. However, if the error terms do not exhibit this constant variance, they are described as heteroscedastic. \n",
    "\n",
    "This inconsistency in the spread or dispersion of errors can lead to biased parameter estimates and inefficient model predictions. Thus, identifying and addressing heteroscedasticity is crucial for improving the accuracy and reliability of statistical models, particularly in predictive analytics and machine learning applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6cd707-9aa7-496a-908a-3be7c5d3d441",
   "metadata": {},
   "source": [
    "# 2. Impact on Regression Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd7614-c54a-4ca8-ad64-9393be9136b0",
   "metadata": {},
   "source": [
    "**Exposing the importance of Heteroskedasticity through its consequences in Statistical Analysis**\n",
    "\n",
    "Heteroskedasticity, while not resulting in biased parameter estimates, leads to several important consequences that affect the efficiency and reliability of Ordinary Least Squares (OLS) estimates:\n",
    "\n",
    "- **Non-BLUE Estimates**: Although heteroskedasticity does not introduce bias into parameter estimates, OLS estimates are no longer Best Linear Unbiased Estimators (BLUE). This means that among all unbiased estimators, OLS does not provide the estimate with the smallest variance. Depending on the nature of heteroskedasticity, significance tests can produce inflated or deflated results. Allison explains this as OLS giving equal weight to all observations, even though those with larger disturbance variance contain less informative value compared to observations with smaller disturbance variance.\n",
    "\n",
    "- **Biased Standard Errors**: Heteroskedasticity also leads to biased standard errors. This bias affects test statistics and confidence intervals derived from OLS estimates, potentially leading to incorrect conclusions about the significance of variables or the overall model.\n",
    "\n",
    "- **Impact on Significance Tests**: Unless heteroskedasticity is severe, significance tests may remain relatively unaffected. This implies that OLS estimation can still be used without significant distortion in most cases. However, severe heteroskedasticity can introduce serious problems, affecting the reliability of parameter estimates and hypothesis testing outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1777f190-6f25-435f-906e-db980ffd810e",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/Heteroskedasticity_overall_effect.jpg\" alt=\"Overall Effect of Heteroskedasticity\" style=\"width: 800px;\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5b742-9d54-4a87-9375-f5a36eae7058",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Causes of Heteroskedasticity:\n",
    "- Errors increase with higher values of independent variables (IVs).\n",
    "> For example, consider a model in which\n",
    "annual family income is the IV and annual family expenditures on vacations is the DV.\n",
    "Families with low incomes will spend relatively little on vacations, and the variations in\n",
    "expenditures across such families will be small. But for families with large incomes, the\n",
    "amount of discretionary income will be higher. The mean amount spent on vacations will be\n",
    "higher, and there will also be greater variability among such families, resulting in\n",
    "heteroskedasticity.\n",
    "\n",
    "<center><img src=\"../images/heteroskedastic_dataset_graph.png\" alt=\"Heteroskedasticy: visual representation\" style=\"width: 800px;\"/></center> \n",
    "\n",
    "**Note that, in this example, a high family income is a necessary but not sufficient condition\n",
    "for large vacation expenditures. Any time a high value for an IV is a necessary but not\n",
    "sufficient condition for an observation to have a high value on a DV, heteroskedasticity is\n",
    "likely.**\n",
    "\n",
    "- Errors may also increase with extreme values of IVs in either direction, e.g. with attitudes that range from extremely negative to extremely positive. This will produce something that looks like an hourglass shape.\n",
    "\n",
    "<center><img src=\"../images/Hour_glass_shape_scatter_plot.png\" alt=\"Hour Glass shape scatter plot of residuals\" style=\"width: 800px;\"/></center> \n",
    "\n",
    "- Measurement error in data collection can contribute to heteroskedasticity, e,g. some respondents might provide more accurate responses than others. (**Note** that this problem arises from the violation of another assumption, that variables are measured without error.)\n",
    "- Subpopulation differences or interaction effects can lead to heteroskedasticity (e.g. the effect of income on expenditures differs for whites and blacks). (Again, the problem arises from violation of the assumption that no such differences exist or have already been incorporated into the model.)\n",
    "> For example, in the following diagram suppose that **Z** stands for three different populations. At low values of *X*, the regression lines for each population are very close to each other. As *X* gets bigger, the regression lines get further and further apart. This means that the residual values will also get further and further apart.\n",
    "\n",
    "<center><img src=\"../images/subpopulation_differences.png\" alt=\"subpopulation differences in the dataset\" style=\"width: 800px;\"/></center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d138a2-47c4-43d3-99e9-1ebed7f06363",
   "metadata": {},
   "source": [
    "# 3. Detecting Heteroscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa449a-5459-48aa-a133-3ee29dfa9bc3",
   "metadata": {},
   "source": [
    "## Visual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f80a5-b518-43f3-839b-74d7a2b0f4e2",
   "metadata": {},
   "source": [
    "\n",
    "The main visual inspection tool for identifying heteroscedasticity is the residuals plot. However, there are several other visual tools that can provide insights into heteroscedasticity:\n",
    "\n",
    "1. **Residuals Plot:** Shows the residuals against predicted values or independent variables.\n",
    "2. **Scatterplot of Residuals:** Plots residuals against actual data points to identify patterns.\n",
    "3. **Variance Inflation Factor (VIF) Plot:** Indicates multicollinearity among predictor variables.\n",
    "4. **Leverage Plot:** Shows the influence of each data point on the regression model.\n",
    "5. **Standardized Residuals Plot:** Displays standardized residuals against predictor variables or fitted values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b98307-0c17-4ebf-8891-aba80588de89",
   "metadata": {},
   "source": [
    "## Formal tests like Breusch-Pagan test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255b155-641f-4291-bfed-6c5aac244480",
   "metadata": {},
   "source": [
    "In regression analysis, detecting heteroscedasticity is crucial for ensuring the reliability of statistical inference. Several formal tests have been developed to assess heteroscedasticity:\n",
    "\n",
    "### Breusch-Pagan Test: \n",
    "\n",
    "The Breusch-Pagan test evaluates whether the variance of residuals changes systematically with the predictor variables in the model. This test involves the following steps:\n",
    "1. Fit the regression model.\n",
    "2. Obtain the residuals from the model.\n",
    "3. Perform a regression of squared residuals on the independent variables.\n",
    "4. Use the F-statistic from this regression to test for heteroscedasticity.\n",
    "\n",
    "### White's Test: \n",
    "\n",
    "White's test is a robust examination for heteroscedasticity that accounts for potential serial correlation in residuals. The steps include:\n",
    "1. Fit the regression model.\n",
    "2. Obtain both residuals and squared residuals.\n",
    "3. Run a regression of squared residuals on independent variables and their cross-products.\n",
    "4. Use the F-statistic from this regression to assess heteroscedasticity.\n",
    "\n",
    "### Goldfeld-Quandt Test: \n",
    "\n",
    "The Goldfeld-Quandt test is particularly useful when investigating heteroscedasticity by comparing variances of residuals between two subgroups of data. The procedure involves:\n",
    "1. Splitting the data into two subgroups based on a criterion.\n",
    "2. Fitting separate regression models to each subgroup.\n",
    "3. Comparing the variances of residuals between subgroups using an appropriate statistical test.\n",
    "\n",
    "### Park Test: \n",
    "\n",
    "Specifically designed for time series data, the Park test examines heteroscedasticity by considering lagged squared residuals. The steps are as follows:\n",
    "1. Fit the time series regression model.\n",
    "2. Obtain the residuals.\n",
    "3. Conduct a regression of squared residuals on lagged squared residuals.\n",
    "4. Utilize the coefficient from this regression to evaluate heteroscedasticity.\n",
    "\n",
    "### Glejser Test: \n",
    "\n",
    "The Glejser test explores the relationship between absolute residuals and predictor variables to detect heteroscedasticity. The steps include:\n",
    "1. Fit the regression model.\n",
    "2. Obtain the residuals.\n",
    "3. Run a regression of absolute residuals on independent variables.\n",
    "4. Use the coefficients from this regression to assess heteroscedasticity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353e48b-59a9-45ec-900e-6fee32c0cf4f",
   "metadata": {},
   "source": [
    "# 4. Addressing Heteroscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ec7d7-99f3-4d0a-9ed1-b7197fdcebca",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593fa82-235e-47da-a397-d57a652e8c28",
   "metadata": {},
   "source": [
    "Transformations modify variable scales or distributions to achieve a more uniform spread of residuals, aligning with regression assumptions. Common methods such as logarithmic, square root, reciprocal, power transformations, and Box-Cox transformations offer distinct strategies based on data characteristics and heteroscedasticity patterns.\n",
    "\n",
    "In this part, we delve into the scientific rationale behind using transformations in heteroscedastic datasets, exploring their intuitive impact and providing step-by-step procedures for implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7aba0b-8494-45c9-848a-eb8347a1395e",
   "metadata": {},
   "source": [
    "### Logarithmic Transformation\n",
    "\n",
    "#### Intuition:\n",
    "- **Purpose:** Logarithmic transformation is employed to stabilize the variance of residuals, particularly when the variance increases exponentially with predictor variables.\n",
    "- **Effect:** By compressing large values and spreading out small values, logarithmic transformation aims to achieve a more consistent spread of residuals across predictor variable values.\n",
    "\n",
    "#### Procedure:\n",
    "1. **Identify Variables:** Determine which predictor variable(s) contribute to heteroscedasticity.\n",
    "2. **Apply Transformation:** Utilize the natural logarithm (ln) or base-10 logarithm (log10) to transform the identified variable(s).\n",
    "3. **Model Adjustment:** Incorporate the transformed variables into your regression model.\n",
    "4. **Validation:** Validate the effectiveness of the transformation through residual plots or formal tests for heteroscedasticity.\n",
    "\n",
    "### Square Root Transformation\n",
    "\n",
    "#### Intuition:\n",
    "- **Purpose:** Square root transformation is suitable for stabilizing variance when it increases with the square of predictor variables.\n",
    "- **Effect:** Taking the square root of a variable helps in moderating extreme values and promoting a more even dispersion of residuals.\n",
    "\n",
    "#### Procedure:\n",
    "1. **Identify Variables:** Determine which variable(s) exhibit heteroscedasticity based on variance trends.\n",
    "2. **Apply Transformation:** Implement the square root transformation to the identified variable(s).\n",
    "3. **Model Integration:** Integrate the transformed variables into your regression model.\n",
    "4. **Evaluation:** Assess the transformation's efficacy using residual analysis or formal heteroscedasticity tests.\n",
    "\n",
    "### Other Transformations\n",
    "- **Reciprocal Transformation (1/x):** Useful for variables with inversely proportional relationships to variance.\n",
    "- **Power Transformations (x^p):** Offers flexibility in adjusting variance based on the power (p) applied.\n",
    "- **Box-Cox Transformation:** A parametric approach that optimizes the transformation to achieve a constant variance.\n",
    "\n",
    "### Considerations\n",
    "- **Interpretation:** Be mindful of the interpretation changes introduced by transformations, especially in terms of coefficients and effect sizes.\n",
    "- **Validation:** Always validate transformations through diagnostic checks to ensure they address heteroscedasticity effectively.\n",
    "- **Theoretical Alignment:** Ensure transformations align with the theoretical understanding of relationships in your data to avoid misleading interpretations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9d92b-acb1-4e5d-b49b-b5f9eaca04ab",
   "metadata": {},
   "source": [
    "## Weighted least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578eee20-5dc0-45a8-8884-2a11bd124b9b",
   "metadata": {},
   "source": [
    "Weighted least squares (WLS) regression is a method used to address heteroscedasticity. This method incorporates weights into the regression model, assigning higher weights to observations with lower variance and lower weights to observations with higher variance. By doing so, WLS aims to mitigate the impact of heteroscedasticity on the estimation process and improve the accuracy of parameter estimates.\n",
    "\n",
    "**Scientific Intuition:**\n",
    "- **Variance Adjustment:** WLS accounts for the varying levels of variance in residuals by adjusting the contribution of each observation based on its estimated variance.\n",
    "- **Efficiency in Estimation:** By giving more weight to observations with lower variance, WLS enhances the efficiency of parameter estimation compared to ordinary least squares (OLS) regression under heteroscedasticity.\n",
    "\n",
    "**Procedure:**\n",
    "1. **Heteroscedasticity Assessment:** Utilize diagnostic methods or formal tests to confirm the presence of heteroscedasticity in the residuals of the OLS regression model.\n",
    "2. **Weight Calculation:**\n",
    "   Compute appropriate weights for each observation using the following formula:\n",
    "   ${Weight}_i = \\frac{1}{\\sigma_i^2}$\n",
    "   where $( \\sigma_i^2)$ represents the estimated variance for observation $( i )$. This weight calculation assigns higher weights to observations with lower variance $( \\sigma_i^2 )$, reflecting their greater influence in the regression model. Lower variance observations contribute more significantly to parameter estimation in weighted least squares (WLS) regression, effectively addressing heteroscedasticity.\n",
    "\n",
    "4. **Weighted Regression:** Implement the weighted regression using the WLS method, minimizing the sum of squared weighted residuals to account for varying impact of observations.\n",
    "5. **Model Validation:** Validate the weighted regression model using diagnostic procedures such as residual plots to ensure effective mitigation of heteroscedasticity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c4bb6-99d1-405f-8b28-656332a0ac73",
   "metadata": {},
   "source": [
    "# 5. Example with Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600716d-01cf-496f-bbdc-14cc79fb39fa",
   "metadata": {},
   "source": [
    "Importing our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c13426-179c-45ac-9785-f4229fe7431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas  as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1393d3-483b-4a9f-aebb-0ac1469298f9",
   "metadata": {},
   "source": [
    "## Importing our data into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a5cc8-b33a-44b8-8dc9-5df2b221936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/home_price.csv') # csv file is in the my repo inside the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e259ce-083b-4edb-96c2-e48d7bb01142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304deea-de56-462f-a252-3f06cf47f972",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e0e27-54ba-45ff-8fe5-c9f8847ba37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:0.2f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5409e9-73c9-476d-b8fb-f8c40d7d4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62090d8c-0d4b-43ca-9f9e-66ec1d9229cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "flat_ax = ax.flatten()\n",
    "\n",
    "sns.histplot(data=df, x='Price', kde=True, color='orange', ax=flat_ax[0])\n",
    "sns.histplot(data=df, x='House_size', kde=True, ax=flat_ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c07ea-662f-4baf-94e0-7d9474b6b907",
   "metadata": {},
   "source": [
    "Both features `Price` and `House_Size` look right skewed. This information might help us later in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf9a3c-7f3c-4a5c-9e55-310bc1b02753",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df, x='House_size', y='Price', aspect=2, height=4)\n",
    "plt.xlabel('House size: as Independent variable')\n",
    "plt.ylabel('House Price: as Dependent variable')\n",
    "plt.title('House Price Vs House Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e5b36-ee92-4910-a8d4-7fb4c55dcf60",
   "metadata": {},
   "source": [
    "From the plot above, we can observe that `Price` tend to have intence scattering as the `House_Size` gets larger. At a first glance, we can confirm that residuals will show some heteroskedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70507ef8-d6ae-460c-9f11-24286369ca24",
   "metadata": {},
   "source": [
    "## Model building using OLS from statsmodels library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db6615-b177-48fb-b5a1-675ac26abaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = df.drop(columns='Price')\n",
    "target_vector = df.Price\n",
    "\n",
    "formula_str = target_vector.name + ' ~ ' + ' + '.join(feature_matrix)\n",
    "\n",
    "model = smf.ols(formula=formula_str, data=df)\n",
    "fitted = model.fit()\n",
    "\n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e4822-c380-45ff-abed-2f6b4472af89",
   "metadata": {},
   "source": [
    "### Detect Heteroskedasticity using visual methods: Residuals plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925af71-321e-48e6-a5ec-b05618ecf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "sns.scatterplot(fitted.resid, color='green', s=80, ax=ax)\n",
    "ax.axhline(0, color='red')\n",
    "\n",
    "# ax.axhline(0.18, color='blue')\n",
    "# ax.axhline(-0.18, color='blue')\n",
    "\n",
    "plt.title('Residuals vs. Fitted Values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d1af8-d2d3-4063-9c13-32b2530d80cd",
   "metadata": {},
   "source": [
    "### Detect Heteroskedasticity using formal test:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16feb4d-ee98-4922-8eda-447da14c9965",
   "metadata": {},
   "source": [
    "The null hypothesis $( H_0 )$ in the context of heteroscedasticity tests is typically formulated as follows:\n",
    "\n",
    "$H_0: \\text{The errors/residuals exhibit homoscedasticity (constant variance).}$\n",
    "\n",
    "In simpler terms, the null hypothesis states that there is no systematic difference in the variance of errors across different levels or values of the independent variables. If the p-value from the heteroscedasticity test is sufficiently small, we reject this null hypothesis in favor of the alternative hypothesis, indicating that there is evidence of heteroscedasticity present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf6419-95dc-4ae2-930e-f1d85ce24003",
   "metadata": {},
   "source": [
    "To determine whether to reject or not reject the null hypothesis $( H_0)$ based on the p-value:\n",
    "\n",
    "1. **Reject $( H_0 )$ (Evidence of Effect):**\n",
    "   - Small p-value (p < α, where α is the significance level): Strong evidence against $( H_0 )$.\n",
    "   - Indicates that observed data is unlikely under the assumption of $( H_0 )$.\n",
    "\n",
    "2. **Do Not Reject $( H_0 )$ (Lack of Evidence of Effect):**\n",
    "   - Large p-value (p ≥ α): Not enough evidence to conclude against $( H_0 )$.\n",
    "   - Suggests that observed data is consistent with $( H_0 )$ or does not deviate significantly from expected values under $( H_0 )$.\n",
    "\n",
    "In the context of heteroscedasticity tests:\n",
    "- Small p-value: Evidence of heteroscedasticity; reject the assumption of constant variance.\n",
    "- Large p-value: Insufficient evidence of heteroscedasticity; do not reject the assumption of constant variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7bd4ab-9fc9-4fd7-aaf2-686401f2f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white, het_goldfeldquandt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1b3b0-1c74-4c6c-937b-16f512628456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Breusch-Pagan test\n",
    "bp_test = het_breuschpagan(fitted.resid, fitted.model.exog)\n",
    "print(\"Breusch-Pagan test p-value:\", bp_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56419b8-8884-4d52-b172-34cf48ae7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform White test\n",
    "white_test = het_white(fitted.resid, fitted.model.exog)\n",
    "print(\"White test p-value:\", white_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679031e2-e1ad-4848-9af0-85e4fed2733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "\n",
    "# Splitting the data into two subgroups based on the median of 'House_size'\n",
    "median_house_size = df.House_size.median()\n",
    "group1 = df[df.House_size <= median_house_size]\n",
    "group2 = df[df.House_size > median_house_size]\n",
    "\n",
    "# Fitting separate regression models to each subgroup\n",
    "model1 = OLS(group1.Price, group1.House_size).fit()\n",
    "model2 = OLS(group2.Price, group2.House_size).fit()\n",
    "\n",
    "# Performing Goldfeld-Quandt Test for subgroup 1\n",
    "gq_test_group1 = het_goldfeldquandt(model1.resid, model1.model.exog)\n",
    "print(\"Goldfeld-Quandt Test F-statistic for subgroup 1:\", gq_test_group1[0])\n",
    "print(\"Goldfeld-Quandt Test p-value for subgroup 1:\", gq_test_group1[1])\n",
    "\n",
    "print()\n",
    "\n",
    "# Performing Goldfeld-Quandt Test for subgroup 2\n",
    "gq_test_group2 = het_goldfeldquandt(model2.resid, model2.model.exog)\n",
    "print(\"Goldfeld-Quandt Test F-statistic for subgroup 2:\", gq_test_group2[0])\n",
    "print(\"Goldfeld-Quandt Test p-value for subgroup 2:\", gq_test_group2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc6f94-8778-43b6-928a-46c1ccfc80f4",
   "metadata": {},
   "source": [
    ">A smaller F-statistic indicates that the variances of residuals are relatively smaller or more consistent in Subgroup 1 compared to Subgroup 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59ee4e-598c-46bb-b526-8b14b5ad4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Glejser Test\n",
    "glejser_test = het_goldfeldquandt(fitted.resid, fitted.model.exog)\n",
    "print(\"Glejser Test F-statistic:\", glejser_test[0])\n",
    "print(\"Glejser Test p-value:\", glejser_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba2ab9-c518-40d9-8e6d-7a82dbd5867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model\n",
    "model = OLS(target_vector, feature_matrix).fit()\n",
    "\n",
    "# Obtain the residuals\n",
    "residuals = model.resid\n",
    "\n",
    "# Create absolute residuals\n",
    "abs_residuals = np.abs(residuals)\n",
    "\n",
    "# Run a regression of absolute residuals on independent variables\n",
    "abs_residuals_model = OLS(abs_residuals, feature_matrix).fit()\n",
    "\n",
    "# Use the coefficients from this regression to assess heteroscedasticity\n",
    "glejser_coeff = abs_residuals_model.params['House_size']\n",
    "\n",
    "\n",
    "# Check the p-value of the Glejser test coefficient\n",
    "glejser_pvalue = abs_residuals_model.pvalues['House_size']\n",
    "\n",
    "print(\"Glejser Test Coefficient:\", glejser_coeff)\n",
    "print(\"Glejser Test p-value:\", glejser_pvalue)\n",
    "\n",
    "# Determine significance based on a chosen alpha level (e.g., 0.05)\n",
    "alpha = 0.05\n",
    "if glejser_pvalue < alpha:\n",
    "    print(\"\\nThe Glejser test coefficient is statistically significant.\")\n",
    "else:\n",
    "    print(\"\\nThe Glejser test coefficient is not statistically significant.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9d9d1-00b3-4a16-bec4-db239a7cef00",
   "metadata": {},
   "source": [
    "> The Glejser test coefficient is statistically significant, as indicated by the extremely low p-value (close to zero). This implies a strong and meaningful relationship between the `House_size` variable and the variability of residuals in our regression model. Specifically, the positive Glejser test coefficient suggests that larger values of `House_size` are associated with larger absolute residuals, indicating increasing variability or heteroscedasticity as `House_size` increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf34843-feb4-439f-8b81-2ffdfbbd0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "sns.scatterplot(x=df.House_size, y=abs_residuals, s=80, ax=ax)\n",
    "plt.ylabel('Absolute Residuals')\n",
    "plt.title('Relationship between House Size and Absolute Residuals\\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2b124-3c1b-409d-be27-01b86e87483b",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/differences_log_base10-expo.png\" alt=\"Graph showcases the differences between a log base 10 plot, the exponetial plot and the second Bisectrix\" style=\"width: 500px;\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0455c424-7eb9-4116-8751-bb8fe49618cf",
   "metadata": {},
   "source": [
    "> \"When an exponential trend is observed in the residuals plot, characterized by a changing spread of residuals that increases or decreases exponentially with predicted values, a log transformation is often best suited to stabilize variance and linearize the relationship in regression analysis.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f8c86c-bb4c-453a-a090-5d8e27e967d8",
   "metadata": {},
   "source": [
    "# 6. Dealing with Heteroscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e145f-2807-45ac-84a7-57437a91bcb4",
   "metadata": {},
   "source": [
    "## Logarithmic Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d727c31-902a-4c84-ae52-0e5f0b10c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log = OLS(np.log10(df.Price), np.log10(df.House_size)).fit()\n",
    "\n",
    "print(model_log.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9456d4-0c97-4a09-8bc2-de0ce13d0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "sns.scatterplot(model_log.resid, color='green', s=80, ax=ax)\n",
    "ax.axhline(0, color='red')\n",
    "\n",
    "#ax.axhline(0.2, color='blue')\n",
    "#ax.axhline(-0.2, color='blue')\n",
    "\n",
    "plt.title('Residuals vs. Fitted Values (After Log transformation)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde4e1b1-188e-44b0-9ace-9dc28dcbb07d",
   "metadata": {},
   "source": [
    "Part II untill the next time.\n",
    "\n",
    "~ Y.L"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
